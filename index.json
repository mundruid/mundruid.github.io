
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    [{"authors":null,"categories":null,"content":"Xenia Mountrouidou is a Senior Security Researcher at Cyber adAPT with a versatile experience in academia and industry. She has over 10 years of research experience in network security, machine learning, and data analytics for computer networks. She enjoys writing Python scripts to automate the boring things, finding interesting patterns with machine learning algorithms, and researching novel intrusion detection techniques. Her research interests revolve around network security, Internet of Things, intrusion detection, and machine learning.\nShe has authored scholarly papers in the areas of performance modeling, computer networks, embedded computer architectures, and computer network security. She has presented her work in academic and industry conferences such as USENIX Security, IEEE Big Data, BSides Security, and Interop.\nDownload my resumé .\n","date":1611014400,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1611014400,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"Xenia Mountrouidou is a Senior Security Researcher at Cyber adAPT with a versatile experience in academia and industry. She has over 10 years of research experience in network security, machine learning, and data analytics for computer networks.","tags":null,"title":"Xenia Mountrouidou","type":"authors"},{"authors":[],"categories":null,"content":"","date":1696681800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1696681800,"objectID":"3f15038bd28593c29409f421b93361b9","permalink":"https://mundruid.github.io/talk/harnessing-ml-and-ai-for-next-gen-security-engineering/","publishdate":"2023-10-25T00:00:00Z","relpermalink":"/talk/harnessing-ml-and-ai-for-next-gen-security-engineering/","section":"event","summary":"Next generation cybersecurity data analytics using the power of Generative AI.","tags":[],"title":"Harnessing ML and AI for Next-Gen Security Engineering","type":"event"},{"authors":null,"categories":null,"content":"Exploratory Data Analysis (EDA) is a versatile tool to get to know your data. However, it can be cumbersome, involving complex mathematics to summarize datasets and elaborate code to create good visualizations that give insight into your data. This is why I am excited to introduce you to the package Pandas AI that can perform EDA simply by posing questions to your data in natural language.\nThe code in this blog can be found in the notebook blog_eda_3.ipynb.\n1,000 ft view What? Pandas AI is an open-source package powered by OpenAI API calls and Pandas dataframe abstractions that offer you the ability to explore your data using language models. Thus, Pandas AI is allowing you to pose questions about your data, test your hypothesis, extract features, and even make graphs. Pandas AI can be used with generic functions or your own custom prompting.\nWhy? Many times we need a simple solution for EDA because of time and resources. Pandas AI provides this solution to perform hypothesis testing and summarization without the math. Combining this with skimpy, summarytools, and sweetviz summaries and visualizations, you can finish your EDA within less than 20 lines of code and enjoy the rest of your day!\nHow? Let’s recap the three parts of EDA that we are currently exploring. We are at the last part, putting everything together with AI powerd EDA.\n- Exploratory Data Analysis - Graphical techniques (part 1) - Histograms - Scatter - Box - Autocorrelation - Statistical techniques (part 2) - Location - Scale - Skewness - Randomness - Distribution measures - Outliers - Pandas AI for data exploration (part 3 - we are here) Load data We will use a pretty neat trick to load the data that we created in the previous blog posts that we saved in pkl (read “pickle”) files:\nmirai_df.to_pickle(\u0026#34;../data/blog_eda/mirai.pkl\u0026#34;) benign_df.to_pickle(\u0026#34;../data/blog_eda/benign.pkl\u0026#34;) Where mirai_df and benign_df were storing the packet captures from Mirai botnet and normal network traffic respectively. We read and transformed these into a Pandas dataframe in previous blogs. We can now load this data:\nmirai_df = pd.read_pickle(\u0026#34;../data/blog_eda/mirai.pkl\u0026#34;) benign_df = pd.read_pickle(\u0026#34;../data/blog_eda/benign.pkl\u0026#34;) This is a trick that we use to save time and transfer processed data from notebook to notebook. We will also load the pickles that hold the processed flows transformed into numeric data. If you want to refresh your memory on how we created these pickle files, check out the EDA Part 1, 2 blog.\nmirai_flow_df_numeric = pd.read_pickle(\u0026#34;../data/blog_eda/mirai_flow_numeric.pkl\u0026#34;) benign_flow_df_numeric = pd.read_pickle(\u0026#34;../data/blog_eda/benign_flow_numeric.pkl\u0026#34;) Pandas AI Now we are ready to use the OpenAI API to explore our data with Pandas AI. To this end, you will need to create an OpenAI API key which is free for the first month, and then you will need to pay as you go.\nWe save the API key in a .env file and make sure that we put this in our .gitignore so that we are not leaking secrets to Github/Gitlab. Then use this awesome python-dotenv package to load our secrets:\nfrom dotenv import load_dotenv # finds .env file and loads the vars load_dotenv() openai_api_key = os.getenv(\u0026#34;OPENAI_API_KEY\u0026#34;, \u0026#34;Key not found\u0026#34;) The only line we will need to have in our .env file is the following:\nOPENAI_API_KEY=\u0026lt;your key\u0026gt; Interrogate your data Now we are ready to ask some questions about our data. First let’s create an LLM object that we will use to reference the OpenAI API, and then we will create a PandasAI object that uses the llm:\n# pandasai imports from pandasai.llm.openai import OpenAI from pandasai import PandasAI # Instantiate a LLM llm = OpenAI(api_token=openai_api_key) pandas_ai = PandasAI(llm) The easiest thing to do is to use an out of the box function to clean our data from duplicate and None values:\nfrom pandasai import SmartDataframe mirai_smart = SmartDataframe(df=pd.DataFrame(mirai_df), config={\u0026#34;llm\u0026#34;: llm}) mirai_cleaned_df = mirai_smart.clean_data() benign_smart = SmartDataframe(df=pd.DataFrame(benign_df), config={\u0026#34;llm\u0026#34;: llm}) benign_cleaned_df = benign_smart.clean_data() Above, you can see the power of the LLM over your data. The SmartDataframe object converts our Pandas dataframes to readable objects by the llm, and a simple function such as clean_data() that, behind the scenes, uses prompting to clean up our data from duplicates and None values. Try to print the shape of your dataframes before and after the cleanup, and you will observe that there are fewer lines after the cleanup.\nLet’s get the most popular IPs that the Mirai pcap was using as sources:\ntop_5_source_IPs = pandas_ai( mirai_clean_df, prompt=\u0026#34;Which are the 5 most popular source IP addresses?\u0026#34; ) top_5_source_IPs Top 5 Mirai IPs Let’s now find the most popular services that Mirai was aiming at:\ntop_5_dst_ports = pandas_ai( mirai_clean_df, prompt=\u0026#34;Which are the 5 most popular destination ports?\u0026#34; ) top_5_dst_ports Top 5 Mirai Ports Thus, the method uses actual prompting and not …","date":1692403200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1692403200,"objectID":"89f7eb5140b2e5ec29a97e2dfc27db52","permalink":"https://mundruid.github.io/post/ml-sec-7/","publishdate":"2023-08-19T00:00:00Z","relpermalink":"/post/ml-sec-7/","section":"post","summary":"Exploratory Data Analysis (EDA) is a versatile tool to get to know your data. However, it can be cumbersome, involving complex mathematics to summarize datasets and elaborate code to create good visualizations that give insight into your data.","tags":null,"title":"Exploring the data - AI","type":"post"},{"authors":null,"categories":null,"content":"Graphs are great for visual learners; however, with statistics, you can literally say, “Show me the numbers!”. EDA with statistics offers a dense data summary and tells the story of your data in a condensed way without having to write too much code since there are many existing packages to help you get the job done.\nIn this blog, we will explore the basics of statistics for EDA and a set of Python EDA libraries that combine statistics and visualization.\nThe code in this blog can be found in the notebook blog_eda_2.ipynb.\n1,000 ft view What? Statistical analysis gives a summary of your data by aggregating it in a meaningful quantitative form. One of the caveats is that your data needs to be measurable or converted to metrics to be summarized with statistics. Categorical data, such as protocols, IP addresses, and many more that we encounter in security data that are not actually numbers, needs to be converted in a meaningful way while maintaining their context. We will review some of these methods here and continue reviewing them in the Feature Engineering set of blogs.\nWhy? Statistical EDA is a quick way to get an insight into your data and can tell the story of these data when combined with graphical techniques. Statistical analysis can verify the conjectures that you made using graphs, lead you to new insights, or point out erroneous biases.\nHow? Let’s recap the three parts of EDA that we are currently exploring. We are right in the middle!\n- Exploratory Data Analysis - Graphical techniques (part 1) - Histograms - Scatter - Box - Autocorrelation - Statistical techniques (part 2 - we are here) - Location - Scale - Skewness - Randomness - Distribution measures - Outliers - Pandas AI for data exploration (part 3) EDA Statistical Techniques This is a great opportunity to explore more features of the Pandas dataframes and how we can convert our data into quantitative metrics that we can summarize with statistics.\nPreprocessing Here is where we will do some data preprocessing. Here are the steps:\nRead the packet capture with scapy, the Python swiss knife for processing packet captures.\npcap_reader_mirai = PcapReader(\u0026#34;../data/blog_eda/mirai.pcap\u0026#34;) pcap_reader_benign = PcapReader(\u0026#34;../data/blog_eda/benign.pcapng\u0026#34;) Convert the packet capture to a Pandas dataframe. I created a function that takes as input the PcapReader object and converts it to a dataframe with columns: (timestamp, src_ip, dst_ip, src_port, dst_port, payload, packet_length, protocol). The timestamp indicates the time the packet arrived. The ports and IPs are part of what defines a data flow. The rest of the data is useful for studying network traffic patterns and extracting behavior.\nThe question arises: Why convert our PCAP data to a Pandas dataframe? There are a lot of functions that can be used on a dataframe to perform exploratory data analysis.\ndef pcap_to_dataframe(pcap_reader: PcapReader) -\u0026gt; pd.DataFrame: \u0026#34;\u0026#34;\u0026#34;Converts raw packet capture to a Pandas dataframe. Args: pcap_reader (PcapReader): packet capture read using scapy Returns: pd.DataFrame: dataframe with pcap data \u0026#34;\u0026#34;\u0026#34; # Create an empty list to store the data data = [] # Iterate through the packets in the pcap file for packet in pcap_reader: # Get the source and destination IP addresses if packet.haslayer(IP): src_ip = packet[IP].src dst_ip = packet[IP].dst protocol = packet[IP].proto else: src_ip = None dst_ip = None protocol = None # Get the source and destination ports and payload if packet.haslayer(TCP): src_port = packet[TCP].sport dst_port = packet[TCP].dport payload = str(packet[TCP].payload) packet_len = len(packet[TCP]) elif packet.haslayer(UDP): src_port = packet[UDP].sport dst_port = packet[UDP].dport payload = str(packet[UDP].payload) packet_len = len(packet[UDP]) elif packet.haslayer(ICMP): payload = str(packet[ICMP].payload) packet_len = len(packet[ICMP]) src_port = None dst_port = None else: src_port = None dst_port = None payload = str(packet.payload) packet_len = len(packet) # Append the data to the list data.append( [ packet.time, src_ip, dst_ip, src_port, dst_port, payload, packet_len, protocol, ] ) # Convert the list to a pandas dataframe df = pd.DataFrame( data, columns=[ \u0026#34;Timestamp\u0026#34;, \u0026#34;Source IP\u0026#34;, \u0026#34;Destination IP\u0026#34;, \u0026#34;Source Port\u0026#34;, \u0026#34;Destination Port\u0026#34;, \u0026#34;Payload\u0026#34;, \u0026#34;Packet Length\u0026#34;, \u0026#34;Protocol\u0026#34;, ], ) return df Finally, I wrote a function to convert the Pandas dataframe data to flows based on: (src_ip, dst_ip, src_port, dst_port). In this case, I use the versatile groupby Pandas function, which is equivalent to SQL GROUP BY. Think of this function as a mashup of data based on specific columns, i.e., features, that gives you the opportunity to perform operations in this group. In my case, I iterate through all the packets, i.e., members, of a flow and calculate the total flow length as a sum of packet length and duration by subtracting the min from the max timestamp. At the end, I put the flow data in another Pandas data frame. Note that you may choose to perform your EDA …","date":1683072000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1683072000,"objectID":"46f2c42f26367cffbbdfdb0d9522671f","permalink":"https://mundruid.github.io/post/ml-sec-6/","publishdate":"2023-05-03T00:00:00Z","relpermalink":"/post/ml-sec-6/","section":"post","summary":"Graphs are great for visual learners; however, with statistics, you can literally say, “Show me the numbers!”. EDA with statistics offers a dense data summary and tells the story of your data in a condensed way without having to write too much code since there are many existing packages to help you get the job done.","tags":null,"title":"Exploring the data - Statistics","type":"post"},{"authors":null,"categories":null,"content":"Even though it is not as glamorous as machine learning, data exploration will give you a return on the time that you invest in it. Knowing the story about your data with graphical and mathematical techniques will lead to better feature engineering, model choices, and tuning of these models.\nThis blog aims to give you good applications for data exploration in security, code examples, and additional resources. As always, the code in this blog is in the repo blog_eda_1.\n1,000 ft view What? The concept of Exploratory Data Analysis (EDA) is a fundamental statistical framework. I like the definition from NIST that characterizes EDA as a philosophy of approaching data as a complete unknown entity and identifying the story they are telling us. The goals of EDA are to test assumptions, find outliers, identify important features, choose a model, and tune the model parameters.\nWhy? It is not glorious to make graphs and gather statistics about your data; however, this pays off towards your ML success. Every time I tried to do the cool stuff first, such as feature extraction or modeling, I failed miserably. If you do not understand the underlying structure of the data, you will not be able to pick the proper techniques to engineer features or pick the features you need. Choosing models randomly is also not a good idea in terms of time and computation resources.\nHow? We will split this topic again into multiple posts to enjoy the journey of graphing, statistics, and interesting new ways to explore your data with AI libraries.\n- Exploratory Data Analysis - Graphical techniques (part 1 - we are here) - Histograms - Scatter - Box - Autocorrelation - Statistical techniques (part 2) - Location - Scale - Skewness - Randomness - Distribution measures - Outliers - Pandas AI for data exploration (part 3) Exploratory Data Analysis (EDA) EDA is a philosophy of approaching any unknown dataset with the mindset of telling the story of your data using:\nGraphical Techniques: creating graphs still involves using statistical tools, but it goes one step further by offering a visual representation of the data. Such a representation offers valuable insights, for example: Do we have some weird outliers in our packet captures? We assume that this user is exhibiting unusual behavior; is this true? We think that a decision tree can help us classify malware. Is this a good choice? Statistical Analysis: Statistical analysis is all about the numbers; it involves quantitative techniques to summarize your data. The benefit is that it is faster than creating graphs and can still help you test assumptions. Probability distributions: this is a more advanced statistical way of understanding data. Think of a probability distribution as a list of values that are the odds in a specific scenario. For example, a set of values like (0.5, 0.3, 0.2) may represent the probabilities of your system having no malware (50%), malware (30%), or the system being in an unknown state (20%). EDA Graphical Techniques We start EDA with graphs. There is no real particular order to start your analysis; you could start with statistics because they are easier to code. I personally start with graphical techniques because I am a visual learner, and they help me understand the statistics.\nProblems with graphical techniques Graphical techniques are based on numerical values, and many times our data in cybersecurity is not as conveniently arranged in numerical values. That is why we need to convert it to numerical values. At this point, this is a chicken and egg issue since extracting numerical features from our data involves feature engineering, but EDA is supposed to lead us to this result. In our examples, we will perform minimal feature engineering and work with data we see a lot in security datasets, and I will share my experience on how I approach these.\nHistograms A histogram is a representation of your data with a bar-type graph where each bar (called a bin) represents a category, for example, packet length, and the height of the graph represents frequency, for example, how many packets of this length are in the packet capture. I am starting with this graph because it is fairly easy to create and can give you a good representation of your data distribution and behavioral patterns. You can use a histogram every time you perform “bean counting”. The accuracy of the histogram depends on the size of the bin or the number of bins. The more bins, the more granular; however, granularity may sacrifice trending.\nLet’s take my favorite packet capture, the Mirai botnet traffic capture, and tell the story of our data with histograms. We first load the data into a dataframe, then use a dataframe function to create a histogram very simply:\nimport pandas as pd mirai = pd.read_csv(\u0026#34;../data/blog_eda/mirai.csv\u0026#34;) mirai.hist(column=\u0026#34;Length\u0026#34;, bins=range(50,100)) Pandas Histogram Notice that I used the only numeric data we can plot, the Length column. I used a range to make the trend more clear …","date":1680220800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1680220800,"objectID":"d00f7b87a41509a8a2beeaa8b9e575bd","permalink":"https://mundruid.github.io/post/ml-sec-5/","publishdate":"2023-03-31T00:00:00Z","relpermalink":"/post/ml-sec-5/","section":"post","summary":"Even though it is not as glamorous as machine learning, data exploration will give you a return on the time that you invest in it. Knowing the story about your data with graphical and mathematical techniques will lead to better feature engineering, model choices, and tuning of these models.","tags":null,"title":"Exploring the data - Graphs","type":"post"},{"authors":null,"categories":null,"content":"Data collection, preparation, and cleaning cover 80%–90% of an ML project. Therefore, it is not surprising that we need to cover it in multiple blogs.\nIn the first part, we talked mainly about theory: how we collect data from our systems and how we store it. We started talking about parsing with one of the categories of cybersecurity data, the log type of data. In this second part, we will analyze more code examples useful for plain text parsing and packet captures. We will also introduce the library that will follow us in all future blogs, the Python Pandas. We start with the 1,000-foot view and then dive deep into it.\nAll the code in this blog and following will be found in my repo cyber-ml.\n1,000 ft view Most of the data that I have experienced analyzing are packet captures and binaries, and I am sure many security professionals will relate.\nOur roadmap of what is covered is listed below, along with links to the current and previous blogs.\n- Data classifications - Cybersecurity - Logs/Events (covered in part 1) - syslog - server log - [host event logs](https://csr.lanl.gov/data/2017/) - Unstructured text (part 2) - domains - urls - passwords - attack payloads - Traces (part 2) - pcaps - netflow - Binaries (part 3) - ML (part 3) - Labeled - Unlabeled Pandas The most prevalent tool that data scientists use is the Pandas library.\nThe reason why the Pandas library is the de facto tool for ML is that it offers a structure called a “DataFrame” that is used to organize data in Excel-style tables. The columns of the Dataframe are called Series, by the way, and DataFrames are a bunch of Series concatenated neatly. The killer feature of Pandas is their API, i.e., their functions. We will be using these a lot, so I will not give you the laundry list; however, here is an example of how to read your csv with one line of code:\nimport pandas as pd mirai_scan = pd.read_csv(\u0026#34;../data/blog3/mirai.csv\u0026#34;) mirai_scan Mirai pcap in Pandas DataFrame Here are my three tips for Pandas as a non-data scientist that learned the hard way:\nUse the API documentation and functions when you can. Before you build something, check if there is a function for it. Use lambda functions instead of regular for loops. I will dedicate a part of my exploratory data analysis blogs to this and show you the way I approach lambdas. Be particularly cognizant when you are modifying the shape or index of your Pandas structures, particularly dataframes. More examples will come your way when we explore the data! One of my favorite tutorials for pandas is Illustrated Pandas. You will find a few more at the Going Deeper section.\nText We just discussed syslog parsing; however, there are other forms of text. Here again, I take the approach: if there is a Python package, use it and do not reinvent the wheel.\nDomains My goto Python library for domains is tldextract because it is a simple way to parse your domain without having to use regular expressions.\nimport tldextract import pandas as pd legit_domains = pd.read_csv(\u0026#34;../data/blog3/top-1m.csv\u0026#34;, names=[\u0026#34;domain\u0026#34;]) legit_domains[\u0026#34;tld\u0026#34;] = [tldextract.extract(d).domain for d in legit_domains[\u0026#34;domain\u0026#34;]] TLDs In this example, I am reading a file with 1 million domains and extracting the top-level domain and subdomain.\nURLs If you have payloads with urls, a great package to use is urlextract.\nimport urlextract # Create an instance of URLExtract class url_extractor = urlextract.URLExtract() # Sample text containing URLs to be extracted dns_payload = \u0026#34;www.googleadservices.com: type A, class IN, addr 142.251.32.194\u0026#34; # Extract URLs from the given text extracted_urls = url_extractor.find_urls(dns_payload) print(\u0026#34;Extracted URLs: \u0026#34;, extracted_urls) In this example, I extracted the url from a DNS payload.\nTraces Traces are collected from the network and have a wealth of information. They are packet captures, the pcap type of files, that are not simple text processing techniques that can parse them. The go-to tool for visualizing PCAPs is Wireshark. In the previous example, I used a packet capture that I had exported to a csv file, which can be done with Wireshark using the option “Export packet dissections”. Of course, you only export the data columns that are shown in your wireshark, and you may lose a good amount of information. That is why scapy is your friend in the case of reading and parsing packet captures.\nfrom scapy.all import rdpcap pcap = rdpcap(\u0026#34;../data/blog3/2023-01-23-Google-ad-to-possible-TA505-activity.pcap\u0026#34;) In this example, we read a pcap and store it in a special scapy object, the PacketList. From there, the sky is the limit on what you can do since scapy can manipulate the whole OSI stack for your parsing.\nfrom scapy.layers.inet import IP, UDP dns_count = 0 for packet in pcap: try: # extract IP layer for UDP protocol udp = packet[IP][UDP] # pick your favorite UDP service... it\u0026#39;s always DNS if udp.dport == 53: print(packet) dns_count += 1 except: continue dns_count You can count DNS requests or any other …","date":1677801600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1677801600,"objectID":"e682bec3d25697e9eb47f89a55276d16","permalink":"https://mundruid.github.io/post/ml-sec-4/","publishdate":"2023-03-03T00:00:00Z","relpermalink":"/post/ml-sec-4/","section":"post","summary":"Data collection, preparation, and cleaning cover 80%–90% of an ML project. Therefore, it is not surprising that we need to cover it in multiple blogs.\nIn the first part, we talked mainly about theory: how we collect data from our systems and how we store it.","tags":null,"title":"Show me the data (part 2)","type":"post"},{"authors":[],"categories":null,"content":"","date":1677177000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1677177000,"objectID":"c25e105098da1d31ec13ac8e2210b548","permalink":"https://mundruid.github.io/talk/level-up-your-data-science-workflows-with-vs-code-jupyter-and-chatgpt/","publishdate":"2023-10-25T00:00:00Z","relpermalink":"/talk/level-up-your-data-science-workflows-with-vs-code-jupyter-and-chatgpt/","section":"event","summary":"VS code Python environments and plugins for Data Scientists.","tags":[],"title":"Level up your Data Science workflows with VS Code, Jupyter and ChatGPT","type":"event"},{"authors":null,"categories":null,"content":"You have probably heard this saying many times before: “Garbage in, garbage out”. This is a big deal in machine learning cybersecurity projects because the model can only be as good as the data you feed it. That is why we are starting with… showing you the data. Data collection, preparation, and cleanup are such an important part of an ML project that we will dedicate three blogs to them before we start exploring the data.\nThis blog will includ: 1,000-foot view with links to the blog, then it will analyze the basic data actions: collect and store, and load with examples on how to parse syslogs.\n1,000 ft view Why? First, we need to have a good dataset to apply some ML models, then some good goals. What makes a good dataset? I’m glad you asked! I promise there will be a blog post with code examples, but just to set the foundation of good data here is what makes them:\nCorrectness: I cannot stress that enough! If you have a wrong IP address, such as 278.12345.213.545, it just does not make sense; however, your model will still work. No gaps: missing values can be a menace in your cybersecurity ML project. When we start preparing data, we will see how to fill these gap using sleek Python libraries. No duplicates: because we do not want to re-process the same stuff over and over, let alone that this can bias our results. Structure: unstructured data is tough (but not impossible) to process. Performance also takes a big hit with unstructured data. Data may be organized in Elastic Docs, spreadsheets, SQL databases, etc. What? The diagram below gives you the types of cyberdata with links to the current blog. Note that some of these will be analyzed in parts 2 and 3:\n- Data classifications - Cybersecurity - Logs/Events - [syslog]() - server log - host event logs - Unstructured text (part 2) - domains - urls - passwords - attack payloads - Traces (part 2) - pcaps - netflow - Binaries (part 3) - ML (part 3) - Labeled - Unlabeled How? The actions for data revolve around collection, storage, and processing. The map below has links to the blog if you want to get into the details:\n- [Collect]() - Monitoring - Observability - Metadata - Public datasets - [Store]() - Time series - Structured - Unstructured - [Load for processing]() Collect \u0026amp; Store In this section, I will explain the collection and storage techniques and technologies. As you will see, you are probably already collecting data with your SIEM and observability tools, and most organizations store their data in a database.\nCollect Definitions: monitoring vs observability First, let’s clarify that monitoring and observability are not the same thing. These terms are often confused. Monitoring is the process of collecting, analyzing, and using information to track events and make educated decisions. Observability is the ability to collect a system’s internal state through logs, traces, and metrics.\nExamples of monitoring \u0026amp; observability A Security Information and Event Management System (SIEM), such as Splunk and Elastic Security, performs monitoring and collects, analyzes, and uses security data for you. An Intrusion Detection System produces observability data for the internal states of your system. Suricata, Zeek, and Snort are good examples of IDSs that produce logs about the state of the system. Syslogs are another example of observable system state. These are ways you are collecting data and you can use them in ML projects.\nMetadata Metadata describes your data. A malware sandbox produces metadata, such as static analysis data, strings, processes, etc. and dynamic analysis data for the malware under analysis. Some tools for security metadata that are open scource and I recommend checking out are: Cuckoo, LiSa for IoT, and MobSF for Android apps.\nPublic datasets Finally, there is a wealth of public repositories that you can use in your ML projects to train them, i.e., teach them to recognize patterns. A few that I recommend (and more in the resources at the end of the blog):\nAwesome cybersecurity datasets: a curated list of different types of datasets grouped in categories such as malware, passwords, etc. SecRepo: grouped data in categories such as network. CAIDA: not security data and specific to network, however the largest network telescope data collection that you can use to train your models. Store There are a few different types of databases for your monitoring and observability data:\nTime Series: these revolve around real-time data, and they use as their “primary key” a timestamp that marks when the data was recorded. These types of databases are preferred for monitoring and the observability of data. SQL: Structured Query Language-type databases are often parallelized to Excel spreadsheets, and they offer standardized querying, performance, and atomicity. Non-SQL: unstructured databases may also be a choice for monitoring data because of the speed of retrieval when it comes to real-time data streams. Load cybersecurity data In this section, I will …","date":1676505600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1676505600,"objectID":"05f74a0eb31e8fdffae90389f29303bf","permalink":"https://mundruid.github.io/post/ml-sec-3/","publishdate":"2023-02-16T00:00:00Z","relpermalink":"/post/ml-sec-3/","section":"post","summary":"You have probably heard this saying many times before: “Garbage in, garbage out”. This is a big deal in machine learning cybersecurity projects because the model can only be as good as the data you feed it.","tags":null,"title":"Show me the data (part 1)","type":"post"},{"authors":null,"categories":null,"content":"At the beginning of our journey to ML and cybersecurity, we need to lay the foundations for a good development environment that fits our needs. As promised, we will start with the big picture of this blog, a quick read, and a roadmap if you do not have time to read the whole post. Then we will discuss how to use three different environments: Google Colaboratory (Colab), Terminal (believe it or not!), and VS Code, the Cadillac of IDES (IMHO). Finally, we will discuss advanced topics for development environments, such as handling secrets securely with Jupyter and pyenv to manage multiple Python versions.\nTable of Contents Prerequisites 1,000 ft View Why? What? How? A Tale of Three Development Environments Google Colab Install packages in Colab Colab options Your favorite editor and a terminal Virtual Environments IPython VS Code and Jupyter Troubleshooting Advanced Topics Storing your secrets securely with Jupyter Notebooks Pyenv Recap Going Even Deeper Did you find this page helpful? Consider sharing it 🙌 Prerequisites In this blog, I assume the following:\nYou have used Python and have basic knowledge of the language keywords and structure, You have installed at least one Python package using pip, You have basic knowledge of Linux command line. If you have never used Python before, here are a couple of free courses that I recommend:\nUdemy: Python from beginer to Intermediate - 30 mins FreeCodeCamp: Python for Beginners – Full Course - 4 hours 1,000 ft View Let’s quickly review the development environments where we will be writing ML software that solves cybersecurity problems.\nWhy? Even though there are several great tools and platforms that embed ML, such as Elastic, Datadog, and Splunk, they still have a lot of limitations in how you can handle your data. Nothing is more adaptable and versatile than the code you created based on your requirements.\nI personally use my development environment in the beginning for data exploration, modeling, and feature storage. Then I deploy a known platform for scaling and ML operations. It pays off in the long run to have your own custom scripts, and they are reusable in multiple projects.\nThe ML development environment is different from what most security professionals use. It centers around Jupyter notebooks, a quick way to run and visualize parts of the script, instead of complete Bash or PowerShell scripts.\nWhat? I suggest three options, depending on your time and the effort you want to put in:\nGoogle colab: takes the least amount of time to set up, is the least flexible, and may infringe on company IP issues. Terminal and favorite editor: medium amount of time, flexible, not a significant productivity boost. The links reference my favorite terminal, Terminator, and editor, Vim. VS Code: takes the most amount of time to set up, offers unlimited flexibility and a huge productivity boost with plugins for remote work and collaboration (ssh plugin, Live Share), and even code generation. How? The diagram shows you the roadmap for each environment, with links on how to set it up. If you want to hear about my experience setting up, based on tears and mess-ups, keep reading!\n- Development Environment - [Google Colab](https://colab.research.google.com/) - Create account - [Install packages in Colab](#install-packages-in-colab) - [Colab options](#colab-options) - Terminal and Editor - [Virtual Environments](#virtual-environments) - [IPython](https://ipython.org/) - VS Code - Extensions - [Python](https://marketplace.visualstudio.com/items?itemName=ms-python.python) - [Jupyter](https://marketplace.visualstudio.com/items?itemName=ms-toolsai.jupyter) - [Setup Jupyter virtual env](#troubleshooting) A Tale of Three Development Environments This is an interesting story for me. I most recently started using Google Colab due to my data science colleagues. My actual first development environment is more traditional for a security professional: a terminal and editor. After I moved to VS Code, I never looked back.\nYou can read each part individually and just set up whichever environment you are more interested in.\nGoogle Colab Colab is straightforward to set up and use. You need to have a Google account, and that is just about it. There is a set of resources to get you started. The benefits of using Colab are that you do not have to manage the Python packages, it has GPU setup, it is quick to set up, and live collaboration is seamless.\nI have created an example, hello_colab.ipynb. The example imports the necessary library, then creates and manipulates dataframe, an artifact that looks like a spreadsheet, from one of the most popular ML libraries, pandas. All you have to do is hover on top of a cell and add new code cells with your code. You can also add markdown text. The functionalities of Colab Notebooks are the same as Jupyter Notebooks. See the image below on how to add Code, Text (Markdown), and how to run your cell.\nColab Basics. Install packages in Colab How do you install a …","date":1673049600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1673049600,"objectID":"426327d35b9c6084badd929d2e5353f3","permalink":"https://mundruid.github.io/post/ml-sec-2/","publishdate":"2023-01-07T00:00:00Z","relpermalink":"/post/ml-sec-2/","section":"post","summary":"At the beginning of our journey to ML and cybersecurity, we need to lay the foundations for a good development environment that fits our needs. As promised, we will start with the big picture of this blog, a quick read, and a roadmap if you do not have time to read the whole post.","tags":null,"title":"Building a bridge between ML and AI with a development environment","type":"post"},{"authors":null,"categories":null,"content":"Machine Learning (ML) meets Cybersecurity: a match made in heaven or yet another tech hype? Indeed, an interesting combination worth exploring for the security professional who wants to solve their problems in a non traditional manner or for the data scientist that wants to be involved in a significant impact area. Where does one start though? This series of blogs aims to give you a roadmap, tools, inspiration, and technical knowledge to combine these two areas.\nThis is a blog series that has been inspired from my personal background as a researcher. I enjoy both the ML and Cybersecurity areas and find it difficult to chose one over the another. I am passionate about conveying knowledge, especially theory and math, in an intuitive manner.\nWhy ML \u0026amp; Cybersecurity? What reasons do you have to dive into ML and Cybersecurity, other than these being hot technical areas? First, as technical professionals, we have to become at some point data scientists, because we have to handle data and need make sense of it. In the security area, thanks to advances in observability, fast time series databases, as well as cheap storage, we have lots of data accumulating and unexplored. These are a treasure trove of information for potential vulnerabilities of our systems as long as we can decipher the patterns. In addition to all the syslogs there is the network, a large data producer. The network produces rich packet captures that we only observe closely when there is a problem. Finally, how about the unknowns? Unknown malware, unknown exploits, and zero days can be recognized with good machine learning techniques.\nIntersection between Cybersecurity \u0026amp; ML There is some intersection between Cybersecurity and ML that can help make the learning curve smoother. The figure above is by no means an exhaustive list of topics. What other topics can you think that overlap (or not) between Cybersecurity and ML?\nWhat this blog series is The prevalent theme in the blogs is the data approach to solving problems for cybersecurity as you can observe in the diagram below. We will be taking this journey from the perspective of the security analyst with examples, concepts, and problems inspired by the field.\n- ML \u0026amp; Cybersecurity - The basics - Dev environment zero to hero - Show me the data! - Let\u0026#39;s explore: Exploratory Data Analysis (EDA) - Data as a black box - The maths - The more advanced maths - Do you have a problem? Problem formulation - The data way - The cyber way - A hybrid approach - Feature Engineering - Quantitative - Qualitative - Text - Extracting features - ML Algorithms for Cybersecurity - Anomaly Detection - Classification - Forecasting What this blog series is not This is not an online class or a solution to all security problems using ML. There are a lot of good quality, free classes for ML. I would personally recommend the Google Foundational ML courses and Andrew Ng’s classes. There is also a plethora of great, free courses for security, such as the MIT Computer Systems Security, Web Security, and Responsible Red Teaming to name a few.\nThe combination of ML to solve Cybersecurity problems from the perspective of the security analyst is not something I found in classes or blogs, therefore this blog series aims to guide someone that wants to get started and dive deeper in these two areas.\nWho is this blog for The audience for this blog is the security professional that wants to venture beyond intrusion detection systems and traditional tools. The posts will be addressed to security professionals that have a need to solve their day to day problems, such as detection, prevention, and root cause analysis with out of box solutions. Curious data scientists may also find these blogs interesting and discover a new area for problem solving.\nStructure of the blogs These blog posts will include the following parts:\n1,000 feet view: if you do not have time to read all the details, I will include executive summaries with enough content that you can get started, Technical detail view: Code examples: I will include working code examples and a reference in Github, Video: I will include a video for visual and auditory learners, Researcher view: Math and algorithm explanations: if you are still awake, you can read the explanation of the math and algorithms, Related research: a review of the most prevalent research will be included. Going even deeper: code, tools, and other resources, such as academic papers and books, will be provided at the end, so that you can go deeper into the topics. Where do we go from here? I hope you are as excited as I am in starting this journey. First things first though, we need a development environment that fits each one of us. We will discuss this in detail in the next post.\n– Xenia\nGoing even deeper ML Courses Google Foundational ML courses: I have taken most of these courses, always something to learn even if you are advanced data scientist. Andrew Ng’s classes Awesome ML Courses Cybersecurity Courses: MIT …","date":1673049600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1673049600,"objectID":"00d99d90c3b4a7af2eeb4bf34edde4e0","permalink":"https://mundruid.github.io/post/ml-sec-1/","publishdate":"2023-01-07T00:00:00Z","relpermalink":"/post/ml-sec-1/","section":"post","summary":"Machine Learning (ML) meets Cybersecurity: a match made in heaven or yet another tech hype? Indeed, an interesting combination worth exploring for the security professional who wants to solve their problems in a non traditional manner or for the data scientist that wants to be involved in a significant impact area.","tags":null,"title":"Machine Learning Meets Cybersecurity","type":"post"},{"authors":null,"categories":null,"content":"","date":1666569600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1666569600,"objectID":"d1bd078f1aa9cb47c96ed2a6d5392e99","permalink":"https://mundruid.github.io/not_index/","publishdate":"2022-10-24T00:00:00Z","relpermalink":"/not_index/","section":"","summary":"","tags":null,"title":"","type":"landing"},{"authors":[],"categories":null,"content":"","date":1661605200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1661605200,"objectID":"4fd91fbd9ee72d1b9a00195d02fc20d1","permalink":"https://mundruid.github.io/talk/iot-spy-observability-and-alerting-for-internet-of-things-iot-security/","publishdate":"2022-07-20T00:00:00Z","relpermalink":"/talk/iot-spy-observability-and-alerting-for-internet-of-things-iot-security/","section":"event","summary":"Observability vs monitoring and how observability is used for security with IoT spy.","tags":[],"title":"IoT Spy: Observability and Alerting for Internet of Things (IoT) Security","type":"event"},{"authors":["Xenia Mountrouidou"],"categories":["Blog"],"content":"","date":1658188800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1658188800,"objectID":"3b52de44a428362d4c2df87197ff3bb7","permalink":"https://mundruid.github.io/post/alerting/","publishdate":"2022-07-19T00:00:00Z","relpermalink":"/post/alerting/","section":"post","summary":"This is the third part of the telemetry stack introduction that introduces basic concepts of an alerting engine and how to implement these with Prometheus AlertManager. You can read this post in Introduction to a Telemetry Stack - Part 3","tags":["Pandas","Network","Traffic","Data Analytics"],"title":"Introduction to a Telemetry Stack - Part 3","type":"post"},{"authors":["Xenia Mountrouidou"],"categories":["Blog"],"content":"","date":1647129600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1647129600,"objectID":"952e85ef6c77e402214ea6d5dcf5153f","permalink":"https://mundruid.github.io/post/pandas3/","publishdate":"2022-03-13T00:00:00Z","relpermalink":"/post/pandas3/","section":"post","summary":"The third part of 'Intro to Pandas' discusses about Exploratory Data Analysis (EDA) of black box pcap data using the Pandas library. You can read this post in Intro to Pandas (Part 3) - Forecasting the network","tags":["Pandas","Network","Traffic","Data Analytics"],"title":"Forecasting the Network","type":"post"},{"authors":["Xenia Mountrouidou"],"categories":["Blog"],"content":"","date":1642032000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1642032000,"objectID":"abf08f3ffb0c32bbb343109910e56084","permalink":"https://mundruid.github.io/post/pandas2/","publishdate":"2022-01-13T00:00:00Z","relpermalink":"/post/pandas2/","section":"post","summary":"The seconf part of 'Intro to Pandas' discusses about Exploratory Data Analysis (EDA) of black box pcap data using the Pandas library. You can read this post in Intro to Pandas (Part 2) - Exploratory data analysis for network traffic","tags":["Pandas","Network","Traffic","Data Analytics"],"title":"Exploratory Data Analysis for Network Traffic","type":"post"},{"authors":["Xenia Mountrouidou"],"categories":["Blog"],"content":"","date":1636761600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1636761600,"objectID":"6e929dc84ed3ef80467b02e64cd2ed64","permalink":"https://mundruid.github.io/post/jupyter/","publishdate":"2021-11-13T00:00:00Z","relpermalink":"/post/jupyter/","section":"post","summary":"In this blog I explain how you can use Jupyter notebooks with Poetry package management for Python. You can read this post in the Jupyter Notebooks for Development","tags":["Jupyter","Poetry"],"title":"Jupyter Notebooks for Development","type":"post"},{"authors":null,"categories":null,"content":"We cannot improve what we cannot measure (Kelvin). How does one quantify the absence of an adversary, a fault, or a weakness in a system? How can we define objective, repeatable, and reproducible metrics that evaluate the security of a network? This task is more complex at a time that the face of networking is changing by interconnecting devices such as webcameras, locks, and lights. These exciting and difficult questions are part of my work in quantitative security evaluation. I analyze data, create predictive models, and stream telemetry with modern open source tools.\n","date":1632700800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1632700800,"objectID":"d1311ddf745551c9e117aa4bb7e28516","permalink":"https://mundruid.github.io/project/external-project/","publishdate":"2021-09-27T00:00:00Z","relpermalink":"/project/external-project/","section":"project","summary":"We cannot improve what we cannot measure (Kelvin). How does one quantify the absence of an adversary, a fault, or a weakness in a system? How can we define objective, repeatable, and reproducible metrics that evaluate the security of a network? This task is more complex at a time that the face of networking is changing by interconnecting devices such as webcameras, locks, and lights. These exciting and difficult questions are part of my work in quantitative security evaluation. I analyze data, create predictive models, and stream telemetry with modern open source tools.","tags":["Network Security","Telemetry","Security Metrics"],"title":"IoT Security Metrics","type":"project"},{"authors":[],"categories":null,"content":"","date":1622552400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1622552400,"objectID":"50fa58dac8584fec174ea7c95f569066","permalink":"https://mundruid.github.io/talk/iot-spy-security-metrics-for-smart-devices-with-telegraf-influxdb-and-grafana/","publishdate":"2021-06-01T00:00:00Z","relpermalink":"/talk/iot-spy-security-metrics-for-smart-devices-with-telegraf-influxdb-and-grafana/","section":"event","summary":"Observability for security metrics of smart devices using Telegraf, InfluxDB, and Grafana.","tags":[],"title":"IoT spy: Security metrics for smart devices with Telegraf, InfluxDB, and Grafana","type":"event"},{"authors":["Xenia Mountrouidou"],"categories":["Blog"],"content":"","date":1620864000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1620864000,"objectID":"cdc18a29480870eae5d55d18b03da947","permalink":"https://mundruid.github.io/post/pandas1/","publishdate":"2021-05-13T00:00:00Z","relpermalink":"/post/pandas1/","section":"post","summary":"Intro to Pandas library and how you can use it for network traffic data analysis. You can read this post in the NTC Blog Introduction to Pandas for Network Development","tags":["Pandas","Network","Traffic","Data Analytics"],"title":"Introduction to Pandas for Network Development","type":"post"},{"authors":["Xenia Mountrouidou"],"categories":["Blog"],"content":"","date":1613174400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1581552000,"objectID":"336cd4b900cb5f6407d05c701936f849","permalink":"https://mundruid.github.io/post/promql/","publishdate":"2021-02-13T00:00:00Z","relpermalink":"/post/promql/","section":"post","summary":"Review of the Prometheus Query Language (PromQL), its value and capabilities of processing time series for Network Automation. You can read this post in the NTC Blog Introduction to PromQL","tags":["PromQL","Network","Automation","Telemetry","Time Series Database"],"title":"Introduction to PromQL","type":"post"},{"authors":["Thomas Setzler","Xenia Mountrouidou"],"categories":null,"content":"","date":1611014400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1611014400,"objectID":"bb3d09611eb75e1ef6f7d4427e32756a","permalink":"https://mundruid.github.io/publication/iot_security_metrics/","publishdate":"2021-01-19T00:00:00Z","relpermalink":"/publication/iot_security_metrics/","section":"publication","summary":"Internet of Things (IoT) devices are ubiquitous, with web cameras, smart refrigerators, and digital assistants appearing in homes, offices, and public spaces. However, these devices are lacking in security measures due to their low time to market and insufficient funding for security research and development. In order to improve the security of IoTs, we have defined novel security metrics based on generic IoT characteristics. Furthermore, we have developed automation for experimentation with IoT devices that results to repeatable and reproducible calculations of security metrics within a realistic IoT testbed. Our results demonstrate that repeatable IoT security measurements are feasible with automation. They prove quantitatively intuitive hypotheses. For example, an large number of inbound / outbound network connections contributes to higher probability of compromise or measuring password strength leads to a robust estimation of IoT security.","tags":[],"title":"IoT Metrics and Automation for Security Evaluation","type":"publication"},{"authors":["Casey Wilson","Xenia Mountrouidou","Anna Little"],"categories":null,"content":"","date":1611014400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1611014400,"objectID":"e3b60021221c0c7e7aa5269e75debc0b","permalink":"https://mundruid.github.io/publication/cyberhunt2019/","publishdate":"2019-12-12T00:00:00Z","relpermalink":"/publication/cyberhunt2019/","section":"publication","summary":"Time as a variable for generating features has been widely overlooked in Intrusion Detection System (IDS) research. Computer and network attacks are time series, where time is an important factor that may affect feature generation, and as a result, classification. Nevertheless, there has been little exploration on how to calibrate time for IDSs and attack classification techniques. In this paper we explore time windows as a technique for generating more effective and descriptive features for attack classification. We suggest a framework for feature generation and selection that uses Recursive Feature Elimination (RFE) and time window exploration. Our initial results when applying this framework indicate that there is up to 47% improvement of F1 scores in attack classification when attack features are generated over a variety of time windows, compared to a single, global time window. We find that features calculated over longer lengths of time may be more useful for detecting attacks than over shorter lengths of time. Our methods seem to be most effective at detecting DDoS attacks, particularly those that occur over medium or long durations of time.","tags":[],"title":"Worth the wait? Time window feature optimization for intrusion detection","type":"publication"},{"authors":[],"categories":null,"content":"","date":1604235600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1604235600,"objectID":"1903c2cab27bc67275f3687c3ed0a01f","permalink":"https://mundruid.github.io/talk/testing-your-python-code/","publishdate":"2020-11-01T00:00:00Z","relpermalink":"/talk/testing-your-python-code/","section":"event","summary":"Types of testing and their application to Network Automation.","tags":[],"title":"Testing your Python Code","type":"event"},{"authors":[],"categories":null,"content":"","date":1572613200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1572613200,"objectID":"0aa99c732e85b0dd3f28293006ffb9ad","permalink":"https://mundruid.github.io/talk/security-metrics-is-this-even-possible/","publishdate":"2020-11-01T00:00:00Z","relpermalink":"/talk/security-metrics-is-this-even-possible/","section":"event","summary":"The endless quest for security metrics.","tags":[],"title":"Security Metrics: Is this even possible?","type":"event"},{"authors":null,"categories":null,"content":"Wowchemy is designed to give technical content creators a seamless experience. You can focus on the content and Wowchemy handles the rest.\nHighlight your code snippets, take notes on math classes, and draw diagrams from textual representation.\nOn this page, you’ll find some examples of the types of technical content that can be rendered with Wowchemy.\nExamples Code Wowchemy supports a Markdown extension for highlighting code syntax. You can customize the styles under the syntax_highlighter option in your config/_default/params.yaml file.\n```python import pandas as pd data = pd.read_csv(\u0026#34;data.csv\u0026#34;) data.head() ``` renders as\nimport pandas as pd data = pd.read_csv(\u0026#34;data.csv\u0026#34;) data.head() Mindmaps Wowchemy supports a Markdown extension for mindmaps.\nSimply insert a Markdown markmap code block and optionally set the height of the mindmap as shown in the example below.\nA simple mindmap defined as a Markdown list:\n```markmap {height=\u0026#34;200px\u0026#34;} - Hugo Modules - wowchemy - wowchemy-plugins-netlify - wowchemy-plugins-netlify-cms - wowchemy-plugins-reveal ``` renders as\n- Hugo Modules - wowchemy - wowchemy-plugins-netlify - wowchemy-plugins-netlify-cms - wowchemy-plugins-reveal A more advanced mindmap with formatting, code blocks, and math:\n```markmap - Mindmaps - Links - [Wowchemy Docs](https://wowchemy.com/docs/) - [Discord Community](https://discord.gg/z8wNYzb) - [GitHub](https://github.com/wowchemy/wowchemy-hugo-themes) - Features - Markdown formatting - **inline** ~~text~~ *styles* - multiline text - `inline code` - ```js console.log(\u0026#39;hello\u0026#39;); console.log(\u0026#39;code block\u0026#39;); ``` - Math: $x = {-b \\pm \\sqrt{b^2-4ac} \\over 2a}$ ``` renders as\n- Mindmaps - Links - [Wowchemy Docs](https://wowchemy.com/docs/) - [Discord Community](https://discord.gg/z8wNYzb) - [GitHub](https://github.com/wowchemy/wowchemy-hugo-themes) - Features - Markdown formatting - **inline** ~~text~~ *styles* - multiline text - `inline code` - ```js console.log(\u0026#39;hello\u0026#39;); console.log(\u0026#39;code block\u0026#39;); ``` - Math: $x = {-b \\pm \\sqrt{b^2-4ac} \\over 2a}$ Charts Wowchemy supports the popular Plotly format for interactive charts.\nSave your Plotly JSON in your page folder, for example line-chart.json, and then add the {{\u0026lt; chart data=\u0026#34;line-chart\u0026#34; \u0026gt;}} shortcode where you would like the chart to appear.\nDemo:\nYou might also find the Plotly JSON Editor useful.\nMath Wowchemy supports a Markdown extension for $\\LaTeX$ math. You can enable this feature by toggling the math option in your config/_default/params.yaml file.\nTo render inline or block math, wrap your LaTeX math with {{\u0026lt; math \u0026gt;}}$...${{\u0026lt; /math \u0026gt;}} or {{\u0026lt; math \u0026gt;}}$$...$${{\u0026lt; /math \u0026gt;}}, respectively. (We wrap the LaTeX math in the Wowchemy math shortcode to prevent Hugo rendering our math as Markdown. The math shortcode is new in v5.5-dev.)\nExample math block:\n{{\u0026lt; math \u0026gt;}} $$ \\gamma_{n} = \\frac{ \\left | \\left (\\mathbf x_{n} - \\mathbf x_{n-1} \\right )^T \\left [\\nabla F (\\mathbf x_{n}) - \\nabla F (\\mathbf x_{n-1}) \\right ] \\right |}{\\left \\|\\nabla F(\\mathbf{x}_{n}) - \\nabla F(\\mathbf{x}_{n-1}) \\right \\|^2} $$ {{\u0026lt; /math \u0026gt;}} renders as\n$$\\gamma_{n} = \\frac{ \\left | \\left (\\mathbf x_{n} - \\mathbf x_{n-1} \\right )^T \\left [\\nabla F (\\mathbf x_{n}) - \\nabla F (\\mathbf x_{n-1}) \\right ] \\right |}{\\left \\|\\nabla F(\\mathbf{x}_{n}) - \\nabla F(\\mathbf{x}_{n-1}) \\right \\|^2}$$ Example inline math {{\u0026lt; math \u0026gt;}}$\\nabla F(\\mathbf{x}_{n})${{\u0026lt; /math \u0026gt;}} renders as $\\nabla F(\\mathbf{x}_{n})$.\nExample multi-line math using the math linebreak (\\\\):\n{{\u0026lt; math \u0026gt;}} $$f(k;p_{0}^{*}) = \\begin{cases}p_{0}^{*} \u0026amp; \\text{if }k=1, \\\\ 1-p_{0}^{*} \u0026amp; \\text{if }k=0.\\end{cases}$$ {{\u0026lt; /math \u0026gt;}} renders as\n$$ f(k;p_{0}^{*}) = \\begin{cases}p_{0}^{*} \u0026amp; \\text{if }k=1, \\\\ 1-p_{0}^{*} \u0026amp; \\text{if }k=0.\\end{cases} $$ Diagrams Wowchemy supports a Markdown extension for diagrams. You can enable this feature by toggling the diagram option in your config/_default/params.toml file or by adding diagram: true to your page front matter.\nAn example flowchart:\n```mermaid graph TD A[Hard] --\u0026gt;|Text| B(Round) B --\u0026gt; C{Decision} C --\u0026gt;|One| D[Result 1] C --\u0026gt;|Two| E[Result 2] ``` renders as\ngraph TD A[Hard] --\u0026gt;|Text| B(Round) B --\u0026gt; C{Decision} C --\u0026gt;|One| D[Result 1] C --\u0026gt;|Two| E[Result 2] An example sequence diagram:\n```mermaid sequenceDiagram Alice-\u0026gt;\u0026gt;John: Hello John, how are you? loop Healthcheck John-\u0026gt;\u0026gt;John: Fight against hypochondria end Note right of John: Rational thoughts! John--\u0026gt;\u0026gt;Alice: Great! John-\u0026gt;\u0026gt;Bob: How about you? Bob--\u0026gt;\u0026gt;John: Jolly good! ``` renders as\nsequenceDiagram Alice-\u0026gt;\u0026gt;John: Hello John, how are you? loop Healthcheck John-\u0026gt;\u0026gt;John: Fight against hypochondria end Note right of John: Rational thoughts! John--\u0026gt;\u0026gt;Alice: Great! John-\u0026gt;\u0026gt;Bob: How about you? Bob--\u0026gt;\u0026gt;John: Jolly good! An example Gantt diagram:\n```mermaid gantt section Section Completed :done, des1, 2014-01-06,2014-01-08 Active :active, des2, 2014-01-07, 3d Parallel 1 : des3, after des1, 1d Parallel 2 : des4, after des1, 1d Parallel 3 : des5, after des3, 1d Parallel 4 : des6, after des4, 1d ``` renders …","date":1562889600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1562889600,"objectID":"07e02bccc368a192a0c76c44918396c3","permalink":"https://mundruid.github.io/post/writing-technical-content/","publishdate":"2019-07-12T00:00:00Z","relpermalink":"/post/writing-technical-content/","section":"post","summary":"Wowchemy is designed to give technical content creators a seamless experience. You can focus on the content and Wowchemy handles the rest.\nHighlight your code snippets, take notes on math classes, and draw diagrams from textual representation.","tags":null,"title":"Writing technical content in Markdown","type":"post"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Wowchemy Wowchemy | Documentation\nFeatures Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides Controls Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026#34;blueberry\u0026#34; if porridge == \u0026#34;blueberry\u0026#34;: print(\u0026#34;Eating...\u0026#34;) Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\nFragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}} Press Space to play!\nOne Two Three A fragment can accept two optional parameters:\nclass: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}} Press the S key to view the speaker notes!\nOnly the speaker can read these notes Press S key to view Themes black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026#34;/media/boards.jpg\u0026#34; \u0026gt;}} {{\u0026lt; slide background-color=\u0026#34;#0000FF\u0026#34; \u0026gt;}} {{\u0026lt; slide class=\u0026#34;my-style\u0026#34; \u0026gt;}} Custom CSS Example Let’s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; } Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"https://mundruid.github.io/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Wowchemy's Slides feature.","tags":[],"title":"Slides","type":"slides"}]